{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy.io\n",
    "from sklearn import preprocessing\n",
    "from sklearn import cross_validation\n",
    "from sklearn import manifold\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fileidxJW11 = scipy.io.loadmat(\"XRMB/DATA/FILEIDX/fileidxJW11.mat\")\n",
    "fileidxJW13 = scipy.io.loadmat(\"XRMB/DATA/FILEIDX/fileidxJW13.mat\")\n",
    "fileidxJW24 = scipy.io.loadmat(\"XRMB/DATA/FILEIDX/fileidxJW24.mat\")\n",
    "fileidxJW30 = scipy.io.loadmat(\"XRMB/DATA/FILEIDX/fileidxJW30.mat\")\n",
    "\n",
    "JW11 = scipy.io.loadmat(\"XRMB/DATA/MAT/JW11[numfr1=7,numfr2=7].mat\")\n",
    "JW13 = scipy.io.loadmat(\"XRMB/DATA/MAT/JW13[numfr1=7,numfr2=7].mat\")\n",
    "JW24 = scipy.io.loadmat(\"XRMB/DATA/MAT/JW24[numfr1=7,numfr2=7].mat\")\n",
    "JW30 = scipy.io.loadmat(\"XRMB/DATA/MAT/JW30[numfr1=7,numfr2=7].mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bigindices', 'Valid_Files', 'indices', 'MFCC', 'Phones', 'bigP', '__header__', '__globals__', 'P', 'bigPhones', 'Words', 'frame_locs', 'Frames', 'X', '__version__', 'SpeakerID', 'Times']\n",
      "(25948, 273)\n",
      "(25948, 112)\n",
      "(25948, 39)\n",
      "(25948, 39)\n",
      "(25948, 273)\n",
      "(15000, 273)\n",
      "(10000, 273)\n",
      "(25948,)\n"
     ]
    }
   ],
   "source": [
    "print JW11.keys()\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "mfcc_features = scaler.fit_transform(preprocessing.normalize(np.transpose(JW11['MFCC'])))\n",
    "articulatory_features = scaler.fit_transform(preprocessing.normalize(np.transpose(JW11['X']).astype(float)))\n",
    "phone_labels = np.transpose(JW11['P'][0])\n",
    "\n",
    "lb = preprocessing.LabelBinarizer() #one hot encoding of labels\n",
    "lb.fit(phone_labels)\n",
    "binarized_labels = lb.transform(phone_labels)\n",
    "\n",
    "n_samples = mfcc_features.shape[0]\n",
    "n_mfcc_features = mfcc_features.shape[1]\n",
    "n_articulatory_features = articulatory_features.shape[1]\n",
    "\n",
    "permutation = np.random.permutation(n_samples)\n",
    "X1 = np.asarray([mfcc_features[i] for i in permutation])\n",
    "X2 = np.asarray([articulatory_features[i] for i in permutation])\n",
    "Y = np.asarray([binarized_labels[i] for i in permutation])\n",
    "Phones = np.asarray([phone_labels[i] for i in permutation])\n",
    "\n",
    "train, dev, test = 25948, 40948, 50948 #15948, 25948, 40948 #use 25948, 40948, 50948\n",
    "\n",
    "X1_tr = X1[0:train, :]\n",
    "X1_dev = X1[train:dev, :]\n",
    "X1_test = X1[dev:test, :]\n",
    "\n",
    "X2_tr = X2[0:train, :]\n",
    "\n",
    "Y_tr = Y[0:train, :]\n",
    "Y_dev = Y[train:dev, :]\n",
    "Y_test = Y[dev:test, :]\n",
    "\n",
    "Phones_tr = Phones[0:train]\n",
    "Phones_dev = Phones[train:dev]\n",
    "Phones_test = Phones[dev:test]\n",
    "\n",
    "baseline_acoustic_tr = X1_tr[:, 118:157]\n",
    "baseline_acoustic_dev = X1_dev[:, 118:157]\n",
    "baseline_acoustic_test = X1_test[:, 118:157]\n",
    "    \n",
    "print X1_tr.shape\n",
    "print X2_tr.shape\n",
    "print Y_tr.shape\n",
    "print baseline_acoustic_tr.shape\n",
    "\n",
    "print X1_tr.shape\n",
    "print X1_dev.shape\n",
    "print X1_test.shape\n",
    "print Phones_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.254333333333\n",
      "1 0.298666666667\n",
      "2 0.317533333333\n",
      "3 0.3332\n",
      "4 0.347\n",
      "5 0.361733333333\n",
      "6 0.372666666667\n",
      "7 0.383133333333\n",
      "8 0.391533333333\n",
      "9 0.399133333333\n",
      "10 0.4054\n",
      "11 0.412333333333\n",
      "12 0.418066666667\n",
      "13 0.4256\n",
      "14 0.432266666667\n",
      "15 0.4382\n",
      "16 0.442533333333\n",
      "17 0.446466666667\n",
      "18 0.4506\n",
      "19 0.454466666667\n",
      "20 0.457266666667\n",
      "21 0.459466666667\n",
      "22 0.461666666667\n",
      "23 0.464133333333\n",
      "24 0.466933333333\n",
      "25 0.469866666667\n",
      "26 0.472533333333\n",
      "27 0.475933333333\n",
      "28 0.4778\n",
      "29 0.4804\n",
      "30 0.4826\n",
      "31 0.485933333333\n",
      "32 0.487533333333\n",
      "33 0.489933333333\n",
      "34 0.491\n",
      "35 0.493533333333\n",
      "36 0.495133333333\n",
      "37 0.496866666667\n",
      "38 0.498466666667\n",
      "39 0.499866666667\n",
      "40 0.501666666667\n",
      "41 0.503266666667\n",
      "42 0.505\n",
      "43 0.506466666667\n",
      "44 0.507466666667\n",
      "45 0.508266666667\n",
      "46 0.5104\n",
      "47 0.512133333333\n",
      "48 0.513\n",
      "49 0.513533333333\n",
      "50 0.5152\n",
      "51 0.516333333333\n",
      "52 0.517333333333\n",
      "53 0.5186\n",
      "54 0.520133333333\n",
      "55 0.521466666667\n",
      "56 0.522866666667\n",
      "57 0.523933333333\n",
      "58 0.525333333333\n",
      "59 0.526266666667\n",
      "60 0.527066666667\n",
      "61 0.528133333333\n",
      "62 0.528666666667\n",
      "63 0.529533333333\n",
      "64 0.530266666667\n",
      "65 0.530666666667\n",
      "66 0.5316\n",
      "67 0.532333333333\n",
      "68 0.533066666667\n",
      "69 0.534066666667\n",
      "70 0.534866666667\n",
      "71 0.5356\n",
      "72 0.536733333333\n",
      "73 0.537533333333\n",
      "74 0.538\n",
      "75 0.539333333333\n",
      "76 0.54\n",
      "77 0.5408\n",
      "78 0.541933333333\n",
      "79 0.542533333333\n",
      "80 0.542866666667\n",
      "81 0.5436\n",
      "82 0.543933333333\n",
      "83 0.5444\n",
      "84 0.544866666667\n",
      "85 0.545133333333\n",
      "86 0.545733333333\n",
      "87 0.5466\n",
      "88 0.547066666667\n",
      "89 0.547333333333\n",
      "90 0.547466666667\n",
      "91 0.548333333333\n",
      "92 0.548333333333\n",
      "93 0.548866666667\n",
      "94 0.549466666667\n",
      "95 0.549733333333\n",
      "96 0.550066666667\n",
      "97 0.550866666667\n",
      "98 0.550933333333\n",
      "99 0.551533333333\n",
      "0.5499\n"
     ]
    }
   ],
   "source": [
    "#linear classifier\n",
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=0.01)) #generate a variable to store matrices, etc\n",
    "    #anything to be updated is a variable, else a constant. placeholders not updated (such as data)\n",
    "\n",
    "def model(X, w): #x placeholder for data, w is weight matrix, \n",
    "    return tf.matmul(X, w) #matmul is feedforward, \n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 273]) #none means unspecified number of instances, is a variable\n",
    "Y = tf.placeholder(\"float\", [None, 39])\n",
    "\n",
    "w = init_weights([273, 39])\n",
    "\n",
    "py_x = model(X, w) #sets node of computation graph\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(py_x, Y)) #cost function for NN\n",
    "train_op = tf.train.GradientDescentOptimizer(0.001).minimize(cost) #train operation, define optimizer\n",
    "predict_op = tf.argmax(py_x, 1) #bc one-hot encoded all labels, output label is one with most weight\n",
    "\n",
    "sess = tf.Session() #could be interactive\n",
    "init = tf.initialize_all_variables() #before evaluations, or specify which variables to be init\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(100): #100 epochs\n",
    "    for start, end in zip(range(0, len(X1_tr), 100), range(100, len(X1_tr), 100)): #batches of 100\n",
    "        sess.run(train_op, feed_dict = {X: X1_tr[start:end], Y: Y_tr[start:end]})\n",
    "        #include feed dictionary to provide actual values in placeholders used in the computation\n",
    "    print i, np.mean(np.argmax(Y_dev, axis=1) == sess.run(predict_op, feed_dict = {X: X1_dev, Y: Y_dev}))\n",
    "    \n",
    "print np.mean(np.argmax(Y_test, axis=1) == sess.run(predict_op, feed_dict = {X: X1_test, Y: Y_test}))\n",
    "#evaluate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " autoencoder training: \n",
      "done training autoencoder\n",
      "logistic regression training: \n",
      "0 0.268126812681\n",
      "1 0.343834383438\n",
      "2 0.425442544254\n",
      "3 0.46904690469\n",
      "4 0.500050005001\n",
      "5 0.5199519952\n",
      "6 0.539453945395\n",
      "7 0.552055205521\n",
      "8 0.57095709571\n",
      "9 0.579657965797\n",
      "10 0.603860386039\n",
      "11 0.614561456146\n",
      "12 0.624862486249\n",
      "13 0.640164016402\n",
      "14 0.637063706371\n",
      "15 0.641664166417\n",
      "16 0.641864186419\n",
      "17 0.663666366637\n",
      "18 0.661666166617\n",
      "19 0.670067006701\n",
      "20 0.67596759676\n",
      "21 0.686668666867\n",
      "22 0.687068706871\n",
      "23 0.687268726873\n",
      "24 0.688768876888\n",
      "25 0.686668666867\n",
      "26 0.705370537054\n",
      "27 0.699669966997\n",
      "28 0.693069306931\n",
      "29 0.694769476948\n",
      "30 0.700670067007\n",
      "31 0.69496949695\n",
      "32 0.706170617062\n",
      "33 0.703870387039\n",
      "34 0.705870587059\n",
      "35 0.708770877088\n",
      "36 0.711771177118\n",
      "37 0.704370437044\n",
      "38 0.707170717072\n",
      "39 0.708370837084\n",
      "40 0.712871287129\n",
      "41 0.701270127013\n",
      "42 0.708470847085\n",
      "43 0.707170717072\n",
      "44 0.704170417042\n",
      "45 0.716471647165\n",
      "46 0.712571257126\n",
      "47 0.719671967197\n",
      "48 0.701270127013\n",
      "49 0.70797079708\n",
      "50 0.725472547255\n",
      "51 0.715171517152\n",
      "52 0.71497149715\n",
      "53 0.715471547155\n",
      "54 0.706170617062\n",
      "55 0.725272527253\n",
      "56 0.726572657266\n",
      "57 0.728372837284\n",
      "58 0.732473247325\n",
      "59 0.718871887189\n",
      "60 0.714671467147\n",
      "61 0.727472747275\n",
      "62 0.731573157316\n",
      "63 0.723172317232\n",
      "64 0.72197219722\n",
      "65 0.723272327233\n",
      "66 0.724172417242\n",
      "67 0.729472947295\n",
      "68 0.723172317232\n",
      "69 0.734673467347\n",
      "70 0.740074007401\n",
      "71 0.719271927193\n",
      "72 0.736773677368\n",
      "73 0.73597359736\n",
      "74 0.734673467347\n",
      "75 0.7399739974\n",
      "76 0.735873587359\n",
      "77 0.728072807281\n",
      "78 0.728672867287\n",
      "79 0.730073007301\n",
      "80 0.726672667267\n",
      "81 0.74397439744\n",
      "82 0.737873787379\n",
      "83 0.733673367337\n",
      "84 0.737573757376\n",
      "85 0.742474247425\n",
      "86 0.729272927293\n",
      "87 0.734373437344\n",
      "88 0.734073407341\n",
      "89 0.717671767177\n",
      "90 0.737173717372\n",
      "91 0.740874087409\n",
      "92 0.739473947395\n",
      "93 0.74697469747\n",
      "94 0.746474647465\n",
      "95 0.737473747375\n",
      "96 0.739873987399\n",
      "97 0.735673567357\n",
      "98 0.737573757376\n",
      "99 0.747774777478\n",
      "100 0.735273527353\n",
      "101 0.745274527453\n",
      "102 0.734873487349\n",
      "103 0.749174917492\n",
      "104 0.728872887289\n",
      "105 0.744874487449\n",
      "106 0.752175217522\n",
      "107 0.753375337534\n",
      "108 0.749374937494\n",
      "109 0.753175317532\n",
      "110 0.730773077308\n",
      "111 0.736073607361\n",
      "112 0.746874687469\n",
      "113 0.745074507451\n",
      "114 0.747574757476\n",
      "115 0.748674867487\n",
      "116 0.744674467447\n",
      "117 0.754875487549\n",
      "118 0.748274827483\n",
      "119 0.740274027403\n",
      "120 0.740574057406\n",
      "121 0.737673767377\n",
      "122 0.74597459746\n",
      "123 0.742074207421\n",
      "124 0.755675567557\n",
      "125 0.753275327533\n",
      "126 0.746774677468\n",
      "127 0.750075007501\n",
      "128 0.742474247425\n",
      "129 0.740774077408\n",
      "130 0.743674367437\n",
      "131 0.728172817282\n",
      "132 0.744074407441\n",
      "133 0.753175317532\n",
      "134 0.736373637364\n",
      "135 0.752875287529\n",
      "136 0.752075207521\n",
      "137 0.751075107511\n",
      "138 0.738073807381\n",
      "139 0.743874387439\n",
      "140 0.738173817382\n",
      "141 0.751275127513\n",
      "142 0.753275327533\n",
      "143 0.750375037504\n",
      "144 0.74897489749\n",
      "145 0.745674567457\n",
      "146 0.757875787579\n",
      "147 0.749574957496\n",
      "148 0.746374637464\n",
      "149 0.744874487449\n",
      "150 0.73897389739\n",
      "151 0.731073107311\n",
      "152 0.742474247425\n",
      "153 0.747074707471\n",
      "154 0.750075007501\n",
      "155 0.758475847585\n",
      "156 0.752675267527\n",
      "157 0.751075107511\n",
      "158 0.753175317532\n",
      "159 0.745374537454\n",
      "160 0.748374837484\n",
      "161 0.755775577558\n",
      "162 0.759675967597\n",
      "163 0.761876187619\n",
      "164 0.766376637664\n",
      "165 0.755775577558\n",
      "166 0.745774577458\n",
      "167 0.74497449745\n",
      "168 0.744274427443\n",
      "169 0.744074407441\n",
      "170 0.749174917492\n",
      "171 0.742674267427\n",
      "172 0.740274027403\n",
      "173 0.741874187419\n",
      "174 0.757575757576\n",
      "175 0.752775277528\n",
      "176 0.760076007601\n",
      "177 0.745474547455\n",
      "178 0.741274127413\n",
      "179 0.753475347535\n",
      "180 0.746174617462\n",
      "181 0.752175217522\n",
      "182 0.755675567557\n",
      "183 0.74897489749\n",
      "184 0.757575757576\n",
      "185 0.74797479748\n",
      "186 0.752575257526\n",
      "187 0.755275527553\n",
      "188 0.749674967497\n",
      "189 0.746174617462\n",
      "190 0.745074507451\n",
      "191 0.754475447545\n",
      "192 0.758375837584\n",
      "193 0.759475947595\n",
      "194 0.743574357436\n",
      "195 0.757575757576\n",
      "196 0.753175317532\n",
      "197 0.752275227523\n",
      "198 0.755375537554\n",
      "199 0.761576157616\n",
      "200 0.763576357636\n",
      "201 0.75697569757\n",
      "202 0.754175417542\n",
      "203 0.761376137614\n",
      "204 0.75797579758\n",
      "205 0.7499749975\n",
      "206 0.754475447545\n",
      "207 0.756175617562\n",
      "208 0.762876287629\n",
      "209 0.752875287529\n",
      "210 0.744574457446\n",
      "211 0.746274627463\n",
      "212 0.752175217522\n",
      "213 0.75497549755\n",
      "214 0.763076307631\n",
      "215 0.760576057606\n",
      "216 0.756775677568\n",
      "217 0.750875087509\n",
      "218 0.757375737574\n",
      "219 0.760676067607\n",
      "220 0.748674867487\n",
      "221 0.748874887489\n",
      "222 0.742574257426\n",
      "223 0.736673667367\n",
      "224 0.749374937494\n",
      "225 0.753275327533\n",
      "226 0.760576057606\n",
      "227 0.757175717572\n",
      "228 0.754875487549\n",
      "229 0.762376237624\n",
      "230 0.757575757576\n",
      "231 0.762376237624\n",
      "232 0.754175417542\n",
      "233 0.755375537554\n",
      "234 0.753875387539\n",
      "235 0.757175717572\n",
      "236 0.748874887489\n",
      "237 0.7499749975\n",
      "238 0.751475147515\n",
      "239 0.75797579758\n",
      "240 0.752675267527\n",
      "241 0.747374737474\n",
      "242 0.75097509751\n",
      "243 0.749074907491\n",
      "244 0.758375837584\n",
      "245 0.760276027603\n",
      "246 0.761476147615\n",
      "247 0.765776577658\n",
      "248 0.76597659766\n",
      "249 0.764376437644\n",
      "250 0.759575957596\n",
      "251 0.7499749975\n",
      "252 0.752375237524\n",
      "253 0.756875687569\n",
      "254 0.759575957596\n",
      "255 0.749574957496\n",
      "256 0.754575457546\n",
      "257 0.759875987599\n",
      "258 0.759675967597\n",
      "259 0.763176317632\n",
      "260 0.757375737574\n",
      "261 0.754075407541\n",
      "262 0.762176217622\n",
      "263 0.762676267627\n",
      "264 0.759675967597\n",
      "265 0.745274527453\n",
      "266 0.75697569757\n",
      "267 0.763576357636\n",
      "268 0.752275227523\n",
      "269 0.753875387539\n",
      "270 0.753275327533\n",
      "271 0.75697569757\n",
      "272 0.755375537554\n",
      "273 0.763576357636\n",
      "274 0.758475847585\n",
      "275 0.747374737474\n",
      "276 0.751175117512\n",
      "277 0.759575957596\n",
      "278 0.757175717572\n",
      "279 0.755075507551\n",
      "280 0.746874687469\n",
      "281 0.757675767577\n",
      "282 0.762076207621\n",
      "283 0.756475647565\n",
      "284 0.761776177618\n",
      "285 0.76397639764\n",
      "286 0.763576357636\n",
      "287 0.760876087609\n",
      "288 0.764776477648\n",
      "289 0.75697569757\n",
      "290 0.750475047505\n",
      "291 0.758275827583\n",
      "292 0.746574657466\n",
      "293 0.750875087509\n",
      "294 0.753075307531\n",
      "295 0.755475547555\n",
      "296 0.761876187619\n",
      "297 0.760576057606\n",
      "298 0.762876287629\n",
      "299 0.767776777678\n",
      "300 0.759375937594\n",
      "301 0.759075907591\n",
      "302 0.763876387639\n",
      "303 0.751875187519\n",
      "304 0.74497449745\n",
      "305 0.757275727573\n",
      "306 0.758675867587\n",
      "307 0.756275627563\n",
      "308 0.750375037504\n",
      "309 0.746474647465\n",
      "310 0.749274927493\n",
      "311 0.757275727573\n",
      "312 0.76697669767\n",
      "313 0.753275327533\n",
      "314 0.761576157616\n",
      "315 0.772677267727\n",
      "316 0.768476847685\n",
      "317 0.765776577658\n",
      "318 0.767076707671\n",
      "319 0.760276027603\n",
      "320 0.733673367337\n",
      "321 0.745174517452\n",
      "322 0.751175117512\n",
      "323 0.757575757576\n",
      "324 0.76497649765\n",
      "325 0.754575457546\n",
      "326 0.760276027603\n",
      "327 0.761376137614\n",
      "328 0.763176317632\n",
      "329 0.761176117612\n",
      "330 0.75697569757\n",
      "331 0.758075807581\n",
      "332 0.758575857586\n",
      "333 0.75297529753\n",
      "334 0.758475847585\n",
      "335 0.768476847685\n",
      "336 0.760776077608\n",
      "337 0.761076107611\n",
      "338 0.763776377638\n",
      "339 0.746374637464\n",
      "340 0.744274427443\n",
      "341 0.740274027403\n",
      "342 0.753075307531\n",
      "343 0.756875687569\n",
      "344 0.762276227623\n",
      "345 0.759475947595\n",
      "346 0.756775677568\n",
      "347 0.758875887589\n",
      "348 0.76097609761\n",
      "349 0.770377037704\n",
      "350 0.765276527653\n",
      "351 0.767676767677\n",
      "352 0.762076207621\n",
      "353 0.759475947595\n",
      "354 0.753275327533\n",
      "355 0.752075207521\n",
      "356 0.730473047305\n",
      "357 0.752075207521\n",
      "358 0.74897489749\n",
      "359 0.755075507551\n",
      "360 0.74797479748\n",
      "361 0.754675467547\n",
      "362 0.754675467547\n",
      "363 0.755275527553\n",
      "364 0.750375037504\n",
      "365 0.757175717572\n",
      "366 0.763376337634\n",
      "367 0.762376237624\n",
      "368 0.753675367537\n",
      "369 0.753775377538\n",
      "370 0.762176217622\n",
      "371 0.732773277328\n",
      "372 0.754675467547\n",
      "373 0.752875287529\n",
      "374 0.755875587559\n",
      "375 0.752575257526\n",
      "376 0.754575457546\n",
      "377 0.756075607561\n",
      "378 0.742774277428\n",
      "379 0.751375137514\n",
      "380 0.752575257526\n",
      "381 0.759175917592\n",
      "382 0.753275327533\n",
      "383 0.761376137614\n",
      "384 0.75597559756\n",
      "385 0.761076107611\n",
      "386 0.761376137614\n",
      "387 0.749374937494\n",
      "388 0.757675767577\n",
      "389 0.760576057606\n",
      "390 0.76497649765\n",
      "391 0.759075907591\n",
      "392 0.764376437644\n",
      "393 0.761576157616\n",
      "394 0.75097509751\n",
      "395 0.749474947495\n",
      "396 0.762276227623\n",
      "397 0.759775977598\n",
      "398 0.754775477548\n",
      "399 0.749274927493\n",
      "400 0.747274727473\n",
      "401 0.75497549755\n",
      "402 0.756875687569\n",
      "403 0.748674867487\n",
      "404 0.757375737574\n",
      "405 0.760676067607\n",
      "406 0.760776077608\n",
      "407 0.756475647565\n",
      "408 0.76097609761\n",
      "409 0.763576357636\n",
      "410 0.760576057606\n",
      "411 0.754275427543\n",
      "412 0.763176317632\n",
      "413 0.757075707571\n",
      "414 0.75397539754\n",
      "415 0.755875587559\n",
      "416 0.760476047605\n",
      "417 0.748174817482\n",
      "418 0.7499749975\n",
      "419 0.750075007501\n",
      "420 0.754075407541\n",
      "421 0.75697569757\n",
      "422 0.751475147515\n",
      "423 0.76797679768\n",
      "424 0.763676367637\n",
      "425 0.767376737674\n",
      "426 0.770877087709\n",
      "427 0.767576757676\n",
      "428 0.750275027503\n",
      "429 0.754875487549\n",
      "430 0.74597459746\n",
      "431 0.754075407541\n",
      "432 0.752575257526\n",
      "433 0.747574757476\n",
      "434 0.755275527553\n",
      "435 0.759775977598\n",
      "436 0.761176117612\n",
      "437 0.755575557556\n",
      "438 0.746174617462\n",
      "439 0.748774877488\n",
      "440 0.751275127513\n",
      "441 0.762076207621\n",
      "442 0.750875087509\n",
      "443 0.755775577558\n",
      "444 0.746674667467\n",
      "445 0.749674967497\n",
      "446 0.75397539754\n",
      "447 0.753375337534\n",
      "448 0.743174317432\n",
      "449 0.750175017502\n",
      "450 0.75197519752\n",
      "451 0.756375637564\n",
      "452 0.753275327533\n",
      "453 0.760176017602\n",
      "454 0.761476147615\n",
      "455 0.762076207621\n",
      "456 0.762576257626\n",
      "457 0.760076007601\n",
      "458 0.754875487549\n",
      "459 0.748374837484\n",
      "460 0.754875487549\n",
      "461 0.746074607461\n",
      "462 0.734373437344\n",
      "463 0.74497449745\n",
      "464 0.75397539754\n",
      "465 0.750175017502\n",
      "466 0.748474847485\n",
      "467 0.75397539754\n",
      "468 0.754175417542\n",
      "469 0.748374837484\n",
      "470 0.759875987599\n",
      "471 0.751375137514\n",
      "472 0.75097509751\n",
      "473 0.753275327533\n",
      "474 0.752275227523\n",
      "475 0.746874687469\n",
      "476 0.752675267527\n",
      "477 0.754875487549\n",
      "478 0.742474247425\n",
      "479 0.742174217422\n",
      "480 0.745874587459\n",
      "481 0.747074707471\n",
      "482 0.754375437544\n",
      "483 0.747874787479\n",
      "484 0.751075107511\n",
      "485 0.749474947495\n",
      "486 0.759075907591\n",
      "487 0.750675067507\n",
      "488 0.749774977498\n",
      "489 0.752275227523\n",
      "490 0.732873287329\n",
      "491 0.7499749975\n",
      "492 0.751675167517\n",
      "493 0.74697469747\n",
      "494 0.75297529753\n",
      "495 0.747574757476\n",
      "496 0.755275527553\n",
      "497 0.74897489749\n",
      "498 0.748474847485\n",
      "499 0.74497449745\n"
     ]
    }
   ],
   "source": [
    "#autoencoder with logreg fine tuning with single hidden layer\n",
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=0.01))\n",
    "\n",
    "def lr_model(X, baseline, w_h, w_o):\n",
    "    final = tf.concat(1, [X, baseline]) #concatenate baseline with learned representation\n",
    "    h = tf.nn.sigmoid(tf.matmul(final, w_h)) #linear classifier on top\n",
    "    return tf.matmul(h, w_o)\n",
    "\n",
    "def encoder(X, enc1_w, enc1_b, enc2_w, enc2_b, enc3_w, enc3_b, enc4_w, enc4_b, p_keep_input, p_keep_hidden):\n",
    "    X = tf.nn.dropout(X, p_keep_input) #give layer and probability to keep input\n",
    "    h1 = tf.nn.relu(tf.matmul(X, enc1_w) + enc1_b)\n",
    "    \n",
    "    h1 = tf.nn.dropout(h1, p_keep_hidden)\n",
    "    h2 = tf.nn.relu(tf.matmul(h1, enc2_w) + enc2_b)\n",
    "    \n",
    "    h2 = tf.nn.dropout(h2, p_keep_hidden)\n",
    "    h3 = tf.nn.relu(tf.matmul(h2, enc3_w) + enc3_b)\n",
    "    \n",
    "    h3 = tf.nn.dropout(h3, p_keep_hidden)\n",
    "    #3 layers with linear output layer\n",
    "    return tf.matmul(h3, enc4_w) + enc4_b\n",
    "    \n",
    "def decoder_X1(h4, dec1_w, dec1_b, dec2_w, dec2_b, dec3_w, dec3_b, dec4_w, dec4_b, p_keep_hidden):\n",
    "    h4 = tf.nn.dropout(h4, p_keep_hidden)\n",
    "    h3 = tf.nn.relu(tf.matmul(h4, dec4_w) + dec4_b)\n",
    "    \n",
    "    h3 = tf.nn.dropout(h3, p_keep_hidden)\n",
    "    h2 = tf.nn.relu(tf.matmul(h3, dec3_w) + dec3_b)\n",
    "    \n",
    "    h2 = tf.nn.dropout(h2, p_keep_hidden)\n",
    "    h1 = tf.nn.relu(tf.matmul(h2, dec2_w) + dec2_b)\n",
    "    \n",
    "    h1 = tf.nn.dropout(h1, p_keep_hidden)\n",
    "    return tf.matmul(h1, dec1_w) + dec1_b\n",
    "\n",
    "def decoder_X2(h4, dec4_w, dec4_b, p_keep_hidden): #just a single layer for view2 reconstruction\n",
    "    h4 = tf.nn.dropout(h4, p_keep_hidden)\n",
    "    return tf.matmul(h4, dec4_w) + dec4_b\n",
    "\n",
    "# def decoder_X2(h4, dec1_w, dec1_b, dec2_w, dec2_b, dec3_w, dec3_b, dec4_w, dec4_b, p_keep_hidden):\n",
    "#     h4 = tf.nn.dropout(h4, p_keep_hidden)\n",
    "#     h3 = tf.nn.sigmoid(tf.matmul(h4, dec4_w) + dec4_b)\n",
    "    \n",
    "#     h3 = tf.nn.dropout(h3, p_keep_hidden)\n",
    "#     h2 = tf.nn.sigmoid(tf.matmul(h3, dec3_w) + dec3_b)\n",
    "    \n",
    "#     h2 = tf.nn.dropout(h2, p_keep_hidden)\n",
    "#     h1 = tf.nn.sigmoid(tf.matmul(h2, dec2_w) + dec2_b)\n",
    "    \n",
    "#     h1 = tf.nn.dropout(h1, p_keep_hidden)\n",
    "#     return tf.nn.sigmoid(tf.matmul(h1, dec1_w) + dec1_b)\n",
    "\n",
    "X1 = tf.placeholder(\"float\", [None, 273])\n",
    "X2 = tf.placeholder(\"float\", [None, 112])\n",
    "Y = tf.placeholder(\"float\", [None, 39])\n",
    "\n",
    "#define widths of network\n",
    "enc1_w = init_weights([273, 300])\n",
    "enc2_w = init_weights([300, 150])\n",
    "enc3_w = init_weights([150, 100])\n",
    "enc4_w = init_weights([100, 50])\n",
    "\n",
    "enc1_b = init_weights([1, 300])\n",
    "enc2_b = init_weights([1, 150])\n",
    "enc3_b = init_weights([1, 100])\n",
    "enc4_b = init_weights([1, 50])\n",
    "\n",
    "dec1_w_x1 = init_weights([300, 273])\n",
    "dec2_w_x1 = init_weights([150, 300])\n",
    "dec3_w_x1 = init_weights([100, 150])\n",
    "dec4_w_x1 = init_weights([50, 100])\n",
    "\n",
    "# dec1_w_x2 = init_weights([90, 112])\n",
    "# dec2_w_x2 = init_weights([65, 90])\n",
    "# dec3_w_x2 = init_weights([65, 65])\n",
    "dec4_w_x2 = init_weights([50, 112])\n",
    "\n",
    "dec1_b_x1 = init_weights([1, 273])\n",
    "dec2_b_x1 = init_weights([1, 300])\n",
    "dec3_b_x1 = init_weights([1, 150])\n",
    "dec4_b_x1 = init_weights([1, 100])\n",
    "\n",
    "# dec1_b_x2 = init_weights([1, 112])\n",
    "# dec2_b_x2 = init_weights([1, 90])\n",
    "# dec3_b_x2 = init_weights([1, 65])\n",
    "dec4_b_x2 = init_weights([1, 112])\n",
    "\n",
    "lr_w = init_weights([88, 50])\n",
    "lr_w_o = init_weights([50, 39])\n",
    "# lr_b = init_weights([1, 50])\n",
    "# lr_b_o = init_weights([1, 39])\n",
    "\n",
    "#dropouts are placeholders variables, but not updated during train\n",
    "\n",
    "p_keep_input = tf.placeholder(\"float\")\n",
    "p_keep_hidden = tf.placeholder(\"float\")\n",
    "\n",
    "#\n",
    "construction_x1 = encoder(X1, enc1_w, enc1_b, enc2_w, enc2_b, enc3_w, enc3_b, enc4_w, enc4_b, p_keep_input, p_keep_hidden)\n",
    "reconstruction_x1 = decoder_X1(construction_x1, dec1_w_x1, dec1_b_x1, dec2_w_x1, dec2_b_x1, dec3_w_x1, dec3_b_x1, dec4_w_x1, dec4_b_x1, p_keep_hidden)\n",
    "# reconstruction_x2 = decoder_X2(construction_x1, dec1_w_x2, dec1_b_x2, dec2_w_x2, dec2_b_x2, dec3_w_x2, dec3_b_x2, dec4_w_x2, dec4_b_x2, p_keep_hidden)\n",
    "reconstruction_x2 = decoder_X2(construction_x1, dec4_w_x2, dec4_b_x2, p_keep_hidden)\n",
    "\n",
    "#sum of reconstruction errors\n",
    "ae_cost = tf.nn.l2_loss(reconstruction_x1 - X1) + tf.nn.l2_loss(reconstruction_x2 - X2)\n",
    "ae_train_op = tf.train.RMSPropOptimizer(1e-4, 0.9).minimize(ae_cost) #momentum based optimizer\n",
    "#ae_train_op = tf.train.AdamOptimizer(0.001).minimize(ae_cost)\n",
    "\n",
    "#fine tuning\n",
    "#all variables in chain updated by backprop\n",
    "baseline = tf.placeholder(\"float\", [None, 38])\n",
    "py_x = lr_model(construction_x1, baseline, lr_w, lr_w_o)\n",
    "lr_cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(py_x, Y))\n",
    "lr_train_op = tf.train.AdamOptimizer(0.001).minimize(lr_cost) #root of chain of updates\n",
    "\n",
    "predict_op = tf.argmax(py_x, 1)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "\n",
    "num_epochs = 500\n",
    "\n",
    "#\n",
    "print \"autoencoder training: \"\n",
    "for i in range(num_epochs):\n",
    "    for start, end in zip(range(0, len(X1_tr), 100), range(100, len(X1_tr), 100)):\n",
    "        sess.run(ae_train_op, feed_dict = {X1: X1_tr[start:end], X2: X2_tr[start:end], Y: Y_tr[start:end], p_keep_input: 0.8, p_keep_hidden: 0.5})\n",
    "    #print \"Epoch \", i, \". Cost = \", sess.run(ae_cost, feed_dict = {X1: X1_tr, X2: X2_tr, Y: Y_tr, p_keep_input: 1.0, p_keep_hidden: 1.0})\n",
    "    #print \"\" + str(i) + \", \",\n",
    "print \"done training autoencoder\"\n",
    "        \n",
    "print \"logistic regression training: \"\n",
    "for i in range(500):\n",
    "    for start, end in zip(range(0, len(X1_tr), 100), range(100, len(X1_tr), 100)):\n",
    "        sess.run(lr_train_op, feed_dict = {X1: X1_tr[start:end], Y: Y_tr[start:end], baseline: baseline_acoustic_tr[start:end], p_keep_input: 1.0, p_keep_hidden: 1.0})\n",
    "    #print \"Epoch \", i, \". cost = \", sess.run(lr_cost, feed_dict = {X1: X1_tr[start:end], Y: Y_tr[start:end], baseline: baseline_acoustic_tr[start:end], p_keep_input: 1.0, p_keep_hidden: 1.0})\n",
    "    print i, np.mean(np.argmax(Y_dev, axis=1) == sess.run(predict_op, feed_dict = {X1: X1_dev, Y: Y_dev, baseline: baseline_acoustic_dev, p_keep_input: 1.0, p_keep_hidden: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autoencoder training: \n",
      "0,  1,  2,  3,  4,  5,  6,  7,  8,  9,  10,  11,  12,  13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99,  done training autoencoder\n",
      "logistic regression training: \n",
      "0 0.636863686369\n",
      "1 0.705170517052\n",
      "2 0.734673467347\n",
      "3 0.749474947495\n",
      "4 0.754375437544\n",
      "5 0.755575557556\n",
      "6 0.753175317532\n",
      "7 0.759675967597\n",
      "8 0.773777377738\n",
      "9 0.765476547655\n",
      "10 0.777077707771\n",
      "11 0.783478347835\n",
      "12 0.7899789979\n",
      "13 0.79097909791\n",
      "14 0.794479447945\n",
      "15 0.791579157916\n",
      "16 0.790579057906\n",
      "17 0.79197919792\n",
      "18 0.788478847885\n",
      "19 0.800480048005\n",
      "20 0.806780678068\n",
      "21 0.805480548055\n",
      "22 0.799579957996\n",
      "23 0.790579057906\n",
      "24 0.794679467947\n",
      "25 0.794779477948\n",
      "26 0.798079807981\n",
      "27 0.796279627963\n",
      "28 0.79897989799\n",
      "29 0.805480548055\n",
      "30 0.796079607961\n",
      "31 0.802580258026\n",
      "32 0.806880688069\n",
      "33 0.808880888089\n",
      "34 0.800780078008\n",
      "35 0.780378037804\n",
      "36 0.789278927893\n",
      "37 0.803280328033\n",
      "38 0.806380638064\n",
      "39 0.80798079808\n",
      "40 0.814381438144\n",
      "41 0.818281828183\n",
      "42 0.809680968097\n",
      "43 0.807280728073\n",
      "44 0.800080008001\n",
      "45 0.788278827883\n",
      "46 0.78797879788\n",
      "47 0.804080408041\n",
      "48 0.806480648065\n",
      "49 0.804280428043\n",
      "50 0.808180818082\n",
      "51 0.81198119812\n",
      "52 0.808780878088\n",
      "53 0.811281128113\n",
      "54 0.811481148115\n",
      "55 0.808180818082\n",
      "56 0.803480348035\n",
      "57 0.790079007901\n",
      "58 0.797579757976\n",
      "59 0.790779077908\n",
      "60 0.799979998\n",
      "61 0.805780578058\n",
      "62 0.805480548055\n",
      "63 0.81098109811\n",
      "64 0.805380538054\n",
      "65 0.803680368037\n",
      "66 0.80798079808\n",
      "67 0.803680368037\n",
      "68 0.805280528053\n",
      "69 0.802180218022\n",
      "70 0.801180118012\n",
      "71 0.795479547955\n",
      "72 0.792279227923\n",
      "73 0.803080308031\n",
      "74 0.800680068007\n",
      "75 0.801480148015\n",
      "76 0.806380638064\n",
      "77 0.802680268027\n",
      "78 0.805280528053\n",
      "79 0.8099809981\n",
      "80 0.804380438044\n",
      "81 0.806580658066\n",
      "82 0.809780978098\n",
      "83 0.803780378038\n",
      "84 0.805580558056\n",
      "85 0.798879887989\n",
      "86 0.803380338034\n",
      "87 0.803680368037\n",
      "88 0.797279727973\n",
      "89 0.80698069807\n",
      "90 0.807880788079\n",
      "91 0.811581158116\n",
      "92 0.813681368137\n",
      "93 0.814881488149\n",
      "94 0.817781778178\n",
      "95 0.81898189819\n",
      "96 0.814281428143\n",
      "97 0.819081908191\n",
      "98 0.818581858186\n",
      "99 0.819481948195\n"
     ]
    }
   ],
   "source": [
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=0.01))\n",
    "\n",
    "def lr_model(X, baseline, w_o):\n",
    "    final = tf.concat(1, [X, baseline])\n",
    "    return tf.matmul(final, w_o)\n",
    "\n",
    "def encoder(X, enc1_w, enc1_b, enc2_w, enc2_b, enc3_w, enc3_b, enc4_w, enc4_b, p_keep_input, p_keep_hidden):\n",
    "    X = tf.nn.dropout(X, p_keep_input)\n",
    "    h1 = tf.nn.relu(tf.matmul(X, enc1_w) + enc1_b)\n",
    "    \n",
    "    h1 = tf.nn.dropout(h1, p_keep_hidden)\n",
    "    h2 = tf.nn.relu(tf.matmul(h1, enc2_w) + enc2_b)\n",
    "    \n",
    "    h2 = tf.nn.dropout(h2, p_keep_hidden)\n",
    "    h3 = tf.nn.relu(tf.matmul(h2, enc3_w) + enc3_b)\n",
    "    \n",
    "    h3 = tf.nn.dropout(h3, p_keep_hidden)\n",
    "    return tf.matmul(h3, enc4_w) + enc4_b\n",
    "    \n",
    "def decoder_X1(h4, dec1_w, dec1_b, dec2_w, dec2_b, dec3_w, dec3_b, dec4_w, dec4_b, p_keep_hidden):\n",
    "    h4 = tf.nn.dropout(h4, p_keep_hidden)\n",
    "    h3 = tf.nn.relu(tf.matmul(h4, dec4_w) + dec4_b)\n",
    "    \n",
    "    h3 = tf.nn.dropout(h3, p_keep_hidden)\n",
    "    h2 = tf.nn.relu(tf.matmul(h3, dec3_w) + dec3_b)\n",
    "    \n",
    "    h2 = tf.nn.dropout(h2, p_keep_hidden)\n",
    "    h1 = tf.nn.relu(tf.matmul(h2, dec2_w) + dec2_b)\n",
    "    \n",
    "    h1 = tf.nn.dropout(h1, p_keep_hidden)\n",
    "    return tf.matmul(h1, dec1_w) + dec1_b\n",
    "\n",
    "def decoder_X2(h4, dec4_w, dec4_b, p_keep_hidden):\n",
    "    h4 = tf.nn.dropout(h4, p_keep_hidden)\n",
    "    return tf.matmul(h4, dec4_w) + dec4_b\n",
    "\n",
    "# def decoder_X2(h4, dec1_w, dec1_b, dec2_w, dec2_b, dec3_w, dec3_b, dec4_w, dec4_b, p_keep_hidden):\n",
    "#     h4 = tf.nn.dropout(h4, p_keep_hidden)\n",
    "#     h3 = tf.nn.sigmoid(tf.matmul(h4, dec4_w) + dec4_b)\n",
    "    \n",
    "#     h3 = tf.nn.dropout(h3, p_keep_hidden)\n",
    "#     h2 = tf.nn.sigmoid(tf.matmul(h3, dec3_w) + dec3_b)\n",
    "    \n",
    "#     h2 = tf.nn.dropout(h2, p_keep_hidden)\n",
    "#     h1 = tf.nn.sigmoid(tf.matmul(h2, dec2_w) + dec2_b)\n",
    "    \n",
    "#     h1 = tf.nn.dropout(h1, p_keep_hidden)\n",
    "#     return tf.nn.sigmoid(tf.matmul(h1, dec1_w) + dec1_b)\n",
    "\n",
    "X1 = tf.placeholder(\"float\", [None, 273])\n",
    "X2 = tf.placeholder(\"float\", [None, 112])\n",
    "Y = tf.placeholder(\"float\", [None, 39])\n",
    "\n",
    "enc1_w = init_weights([273, 500])\n",
    "enc2_w = init_weights([500, 500])\n",
    "enc3_w = init_weights([500, 500])\n",
    "enc4_w = init_weights([500, 50])\n",
    "\n",
    "enc1_b = init_weights([1, 500])\n",
    "enc2_b = init_weights([1, 500])\n",
    "enc3_b = init_weights([1, 500])\n",
    "enc4_b = init_weights([1, 50])\n",
    "\n",
    "dec1_w_x1 = init_weights([500, 273])\n",
    "dec2_w_x1 = init_weights([500, 500])\n",
    "dec3_w_x1 = init_weights([500, 500])\n",
    "dec4_w_x1 = init_weights([50, 500])\n",
    "\n",
    "# dec1_w_x2 = init_weights([90, 112])\n",
    "# dec2_w_x2 = init_weights([65, 90])\n",
    "# dec3_w_x2 = init_weights([65, 65])\n",
    "dec4_w_x2 = init_weights([50, 112])\n",
    "\n",
    "dec1_b_x1 = init_weights([1, 273])\n",
    "dec2_b_x1 = init_weights([1, 500])\n",
    "dec3_b_x1 = init_weights([1, 500])\n",
    "dec4_b_x1 = init_weights([1, 500])\n",
    "\n",
    "# dec1_b_x2 = init_weights([1, 112])\n",
    "# dec2_b_x2 = init_weights([1, 90])\n",
    "# dec3_b_x2 = init_weights([1, 65])\n",
    "dec4_b_x2 = init_weights([1, 112])\n",
    "\n",
    "lr_w_o = init_weights([88, 39])\n",
    "# lr_b = init_weights([1, 50])\n",
    "# lr_b_o = init_weights([1, 39])\n",
    "\n",
    "p_keep_input = tf.placeholder(\"float\")\n",
    "p_keep_hidden = tf.placeholder(\"float\")\n",
    "\n",
    "construction_x1 = encoder(X1, enc1_w, enc1_b, enc2_w, enc2_b, enc3_w, enc3_b, enc4_w, enc4_b, p_keep_input, p_keep_hidden)\n",
    "reconstruction_x1 = decoder_X1(construction_x1, dec1_w_x1, dec1_b_x1, dec2_w_x1, dec2_b_x1, dec3_w_x1, dec3_b_x1, dec4_w_x1, dec4_b_x1, p_keep_hidden)\n",
    "# reconstruction_x2 = decoder_X2(construction_x1, dec1_w_x2, dec1_b_x2, dec2_w_x2, dec2_b_x2, dec3_w_x2, dec3_b_x2, dec4_w_x2, dec4_b_x2, p_keep_hidden)\n",
    "reconstruction_x2 = decoder_X2(construction_x1, dec4_w_x2, dec4_b_x2, p_keep_hidden)\n",
    "\n",
    "ae_cost = tf.nn.l2_loss(reconstruction_x1 - X1) + tf.nn.l2_loss(reconstruction_x2 - X2)\n",
    "ae_train_op = tf.train.RMSPropOptimizer(1e-3, 0.99).minimize(ae_cost)\n",
    "#ae_train_op = tf.train.AdagradOptimizer(1e-3).minimize(ae_cost)\n",
    "\n",
    "baseline = tf.placeholder(\"float\", [None, 38])\n",
    "py_x = lr_model(construction_x1, baseline, lr_w_o)\n",
    "lr_cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(py_x, Y))\n",
    "lr_train_op = tf.train.AdamOptimizer(1e-3).minimize(lr_cost)\n",
    "\n",
    "predict_op = tf.argmax(py_x, 1)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "print \"autoencoder training: \"\n",
    "for i in range(num_epochs):\n",
    "    for start, end in zip(range(0, len(X1_tr), 100), range(100, len(X1_tr), 100)):\n",
    "        sess.run(ae_train_op, feed_dict = {X1: X1_tr[start:end], X2: X2_tr[start:end], Y: Y_tr[start:end], p_keep_input: 0.8, p_keep_hidden: 0.5})\n",
    "    #print \"Epoch \", i, \". Cost = \", sess.run(ae_cost, feed_dict = {X1: X1_tr, X2: X2_tr, Y: Y_tr, p_keep_input: 1.0, p_keep_hidden: 1.0})\n",
    "    print \"\" + str(i) + \", \",\n",
    "print \"done training autoencoder\"\n",
    "        \n",
    "print \"logistic regression training: \"\n",
    "for i in range(num_epochs):\n",
    "    for start, end in zip(range(0, len(X1_tr), 100), range(100, len(X1_tr), 100)):\n",
    "        sess.run(lr_train_op, feed_dict = {X1: X1_tr[start:end], Y: Y_tr[start:end], baseline: baseline_acoustic_tr[start:end], p_keep_input: 1.0, p_keep_hidden: 1.0})\n",
    "    #print \"Epoch \", i, \". cost = \", sess.run(lr_cost, feed_dict = {X1: X1_tr[start:end], Y: Y_tr[start:end], baseline: baseline_acoustic_tr[start:end], p_keep_input: 1.0, p_keep_hidden: 1.0})\n",
    "    print i, np.mean(np.argmax(Y_dev, axis=1) == sess.run(predict_op, feed_dict = {X1: X1_dev, Y: Y_dev, baseline: baseline_acoustic_dev, p_keep_input: 1.0, p_keep_hidden: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " autoencoder training: \n",
      "0,  1,  2,  3,  4,  5,  6,  7,  8,  9,  10,  11,  12,  13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99,  done training autoencoder\n",
      "logistic regression training: \n",
      "0 0.693466666667\n",
      "1 0.753733333333\n",
      "2 0.7756\n",
      "3 0.801733333333\n",
      "4 0.8012\n",
      "5 0.8092\n",
      "6 0.812333333333\n",
      "7 0.817733333333\n",
      "8 0.824\n",
      "9 0.830066666667\n",
      "10 0.831133333333\n",
      "11 0.835866666667\n",
      "12 0.8336\n",
      "13 0.8264\n",
      "14 0.831\n",
      "15 0.827\n",
      "16 0.827933333333\n",
      "17 0.833866666667\n",
      "18 0.840066666667\n",
      "19 0.839666666667\n",
      "20 0.8438\n",
      "21 0.838066666667\n",
      "22 0.848466666667\n",
      "23 0.841333333333\n",
      "24 0.841533333333\n",
      "25 0.837533333333\n",
      "26 0.834933333333\n",
      "27 0.8434\n",
      "28 0.845066666667\n",
      "29 0.8466\n",
      "30 0.848533333333\n",
      "31 0.844666666667\n",
      "32 0.8402\n",
      "33 0.8368\n",
      "34 0.844533333333\n",
      "35 0.846866666667\n",
      "36 0.843666666667\n",
      "37 0.853466666667\n",
      "38 0.851466666667\n",
      "39 0.836866666667\n",
      "40 0.847266666667\n",
      "41 0.843266666667\n",
      "42 0.843266666667\n",
      "43 0.8446\n",
      "44 0.8496\n",
      "45 0.85\n",
      "46 0.8504\n",
      "47 0.8468\n",
      "48 0.847466666667\n",
      "49 0.842133333333\n",
      "50 0.841866666667\n",
      "51 0.846133333333\n",
      "52 0.851466666667\n",
      "53 0.844933333333\n",
      "54 0.8422\n",
      "55 0.840333333333\n",
      "56 0.847133333333\n",
      "57 0.850333333333\n",
      "58 0.849333333333\n",
      "59 0.851666666667\n",
      "60 0.8538\n",
      "61 0.8528\n",
      "62 0.840866666667\n",
      "63 0.843466666667\n",
      "64 0.848533333333\n",
      "65 0.844866666667\n",
      "66 0.8442\n",
      "67 0.852533333333\n",
      "68 0.842466666667\n",
      "69 0.850066666667\n",
      "70 0.8444\n",
      "71 0.839933333333\n",
      "72 0.847333333333\n",
      "73 0.845933333333\n",
      "74 0.842333333333\n",
      "75 0.853\n",
      "76 0.849\n",
      "77 0.841333333333\n",
      "78 0.8544\n",
      "79 0.853133333333\n",
      "80 0.841933333333\n",
      "81 0.845866666667\n",
      "82 0.8484\n",
      "83 0.845733333333\n",
      "84 0.8464\n",
      "85 0.852466666667\n",
      "86 0.850666666667\n",
      "87 0.8414\n",
      "88 0.8448\n",
      "89 0.842933333333\n",
      "90 0.848666666667\n",
      "91 0.854733333333\n",
      "92 0.850066666667\n",
      "93 0.850133333333\n",
      "94 0.847266666667\n",
      "95 0.847133333333\n",
      "96 0.8528\n",
      "97 0.8536\n",
      "98 0.851466666667\n",
      "99 0.850133333333\n"
     ]
    }
   ],
   "source": [
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=0.01))\n",
    "\n",
    "def lr_model(X, baseline, w_o):\n",
    "    final = tf.concat(1, [X, baseline])\n",
    "    return tf.matmul(final, w_o)\n",
    "\n",
    "def encoder(X, enc1_w, enc1_b, enc2_w, enc2_b, enc3_w, enc3_b, enc4_w, enc4_b, p_keep_input, p_keep_hidden):\n",
    "    X = tf.nn.dropout(X, p_keep_input)\n",
    "    h1 = tf.nn.relu(tf.matmul(X, enc1_w) + enc1_b)\n",
    "    \n",
    "    h1 = tf.nn.dropout(h1, p_keep_hidden)\n",
    "    h2 = tf.nn.relu(tf.matmul(h1, enc2_w) + enc2_b)\n",
    "    \n",
    "    h2 = tf.nn.dropout(h2, p_keep_hidden)\n",
    "    h3 = tf.nn.relu(tf.matmul(h2, enc3_w) + enc3_b)\n",
    "    \n",
    "    h3 = tf.nn.dropout(h3, p_keep_hidden)\n",
    "    return tf.matmul(h3, enc4_w) + enc4_b\n",
    "    \n",
    "def decoder_X1(h4, dec1_w, dec1_b, dec2_w, dec2_b, dec3_w, dec3_b, dec4_w, dec4_b, p_keep_hidden):\n",
    "    h4 = tf.nn.dropout(h4, p_keep_hidden)\n",
    "    h3 = tf.nn.relu(tf.matmul(h4, dec4_w) + dec4_b)\n",
    "    \n",
    "    h3 = tf.nn.dropout(h3, p_keep_hidden)\n",
    "    h2 = tf.nn.relu(tf.matmul(h3, dec3_w) + dec3_b)\n",
    "    \n",
    "    h2 = tf.nn.dropout(h2, p_keep_hidden)\n",
    "    h1 = tf.nn.relu(tf.matmul(h2, dec2_w) + dec2_b)\n",
    "    \n",
    "    h1 = tf.nn.dropout(h1, p_keep_hidden)\n",
    "    return tf.matmul(h1, dec1_w) + dec1_b\n",
    "\n",
    "def decoder_X2(h4, dec3_w, dec3_b, dec4_w, dec4_b, p_keep_hidden):\n",
    "    h4 = tf.nn.dropout(h4, p_keep_hidden)\n",
    "    h3 = tf.matmul(h4, dec4_w) + dec4_b\n",
    "    \n",
    "    h3 = tf.nn.dropout(h3, p_keep_hidden)\n",
    "    return tf.matmul(h3, dec3_w) + dec3_b\n",
    "\n",
    "# def decoder_X2(h4, dec1_w, dec1_b, dec2_w, dec2_b, dec3_w, dec3_b, dec4_w, dec4_b, p_keep_hidden):\n",
    "#     h4 = tf.nn.dropout(h4, p_keep_hidden)\n",
    "#     h3 = tf.nn.sigmoid(tf.matmul(h4, dec4_w) + dec4_b)\n",
    "    \n",
    "#     h3 = tf.nn.dropout(h3, p_keep_hidden)\n",
    "#     h2 = tf.nn.sigmoid(tf.matmul(h3, dec3_w) + dec3_b)\n",
    "    \n",
    "#     h2 = tf.nn.dropout(h2, p_keep_hidden)\n",
    "#     h1 = tf.nn.sigmoid(tf.matmul(h2, dec2_w) + dec2_b)\n",
    "    \n",
    "#     h1 = tf.nn.dropout(h1, p_keep_hidden)\n",
    "#     return tf.nn.sigmoid(tf.matmul(h1, dec1_w) + dec1_b)\n",
    "\n",
    "X1 = tf.placeholder(\"float\", [None, 273])\n",
    "X2 = tf.placeholder(\"float\", [None, 112])\n",
    "Y = tf.placeholder(\"float\", [None, 39])\n",
    "\n",
    "enc1_w = init_weights([273, 1500])\n",
    "enc2_w = init_weights([1500, 1500])\n",
    "enc3_w = init_weights([1500, 1500])\n",
    "enc4_w = init_weights([1500, 50])\n",
    "\n",
    "enc1_b = init_weights([1, 1500])\n",
    "enc2_b = init_weights([1, 1500])\n",
    "enc3_b = init_weights([1, 1500])\n",
    "enc4_b = init_weights([1, 50])\n",
    "\n",
    "dec1_w_x1 = init_weights([1500, 273])\n",
    "dec2_w_x1 = init_weights([1500, 1500])\n",
    "dec3_w_x1 = init_weights([1500, 1500])\n",
    "dec4_w_x1 = init_weights([50, 1500])\n",
    "\n",
    "# dec1_w_x2 = init_weights([90, 112])\n",
    "# dec2_w_x2 = init_weights([65, 90])\n",
    "dec3_w_x2 = init_weights([75, 112])\n",
    "dec4_w_x2 = init_weights([50, 75])\n",
    "\n",
    "dec1_b_x1 = init_weights([1, 273])\n",
    "dec2_b_x1 = init_weights([1, 1500])\n",
    "dec3_b_x1 = init_weights([1, 1500])\n",
    "dec4_b_x1 = init_weights([1, 1500])\n",
    "\n",
    "# dec1_b_x2 = init_weights([1, 112])\n",
    "# dec2_b_x2 = init_weights([1, 90])\n",
    "dec3_b_x2 = init_weights([1, 112])\n",
    "dec4_b_x2 = init_weights([1, 75])\n",
    "\n",
    "lr_w_o = init_weights([89, 39])\n",
    "# lr_b = init_weights([1, 50])\n",
    "# lr_b_o = init_weights([1, 39])\n",
    "\n",
    "p_keep_input = tf.placeholder(\"float\")\n",
    "p_keep_hidden = tf.placeholder(\"float\")\n",
    "\n",
    "construction_x1 = encoder(X1, enc1_w, enc1_b, enc2_w, enc2_b, enc3_w, enc3_b, enc4_w, enc4_b, p_keep_input, p_keep_hidden)\n",
    "reconstruction_x1 = decoder_X1(construction_x1, dec1_w_x1, dec1_b_x1, dec2_w_x1, dec2_b_x1, dec3_w_x1, dec3_b_x1, dec4_w_x1, dec4_b_x1, p_keep_hidden)\n",
    "# reconstruction_x2 = decoder_X2(construction_x1, dec1_w_x2, dec1_b_x2, dec2_w_x2, dec2_b_x2, dec3_w_x2, dec3_b_x2, dec4_w_x2, dec4_b_x2, p_keep_hidden)\n",
    "reconstruction_x2 = decoder_X2(construction_x1, dec3_w_x2, dec3_b_x2, dec4_w_x2, dec4_b_x2, p_keep_hidden)\n",
    "\n",
    "ae_cost = tf.nn.l2_loss(reconstruction_x1 - X1) + tf.nn.l2_loss(reconstruction_x2 - X2)\n",
    "ae_train_op = tf.train.RMSPropOptimizer(1e-3, 0.99).minimize(ae_cost)\n",
    "#ae_train_op = tf.train.AdagradOptimizer(1e-3).minimize(ae_cost)\n",
    "\n",
    "baseline = tf.placeholder(\"float\", [None, 39])\n",
    "py_x = lr_model(construction_x1, baseline, lr_w_o)\n",
    "lr_cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(py_x, Y))\n",
    "lr_train_op = tf.train.AdamOptimizer(1e-3).minimize(lr_cost)\n",
    "\n",
    "predict_op = tf.argmax(py_x, 1)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "print \"autoencoder training: \"\n",
    "for i in range(num_epochs):\n",
    "    for start, end in zip(range(0, len(X1_tr), 100), range(100, len(X1_tr), 100)):\n",
    "        sess.run(ae_train_op, feed_dict = {X1: X1_tr[start:end], X2: X2_tr[start:end], Y: Y_tr[start:end], p_keep_input: 0.8, p_keep_hidden: 0.5})\n",
    "    #print \"Epoch \", i, \". Cost = \", sess.run(ae_cost, feed_dict = {X1: X1_tr, X2: X2_tr, Y: Y_tr, p_keep_input: 1.0, p_keep_hidden: 1.0})\n",
    "    print \"\" + str(i) + \", \",\n",
    "print \"done training autoencoder\"\n",
    "        \n",
    "print \"logistic regression training: \"\n",
    "for i in range(num_epochs):\n",
    "    for start, end in zip(range(0, len(X1_tr), 100), range(100, len(X1_tr), 100)):\n",
    "        sess.run(lr_train_op, feed_dict = {X1: X1_tr[start:end], Y: Y_tr[start:end], baseline: baseline_acoustic_tr[start:end], p_keep_input: 1.0, p_keep_hidden: 1.0})\n",
    "    #print \"Epoch \", i, \". cost = \", sess.run(lr_cost, feed_dict = {X1: X1_tr[start:end], Y: Y_tr[start:end], baseline: baseline_acoustic_tr[start:end], p_keep_input: 1.0, p_keep_hidden: 1.0})\n",
    "    print i, np.mean(np.argmax(Y_dev, axis=1) == sess.run(predict_op, feed_dict = {X1: X1_dev, Y: Y_dev, baseline: baseline_acoustic_dev, p_keep_input: 1.0, p_keep_hidden: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def encoder(X, enc1_w, enc1_b, enc2_w, enc2_b, enc3_w, enc3_b, enc4_w, enc4_b, p_keep_input, p_keep_hidden):\n",
    "    X = tf.nn.dropout(X, p_keep_input)\n",
    "    h1 = tf.nn.relu(tf.matmul(X, enc1_w) + enc1_b)\n",
    "    \n",
    "    h1 = tf.nn.dropout(h1, p_keep_hidden)\n",
    "    h2 = tf.nn.relu(tf.matmul(h1, enc2_w) + enc2_b)\n",
    "    \n",
    "    h2 = tf.nn.dropout(h2, p_keep_hidden)\n",
    "    h3 = tf.nn.relu(tf.matmul(h2, enc3_w) + enc3_b)\n",
    "    \n",
    "    h3 = tf.nn.dropout(h3, p_keep_hidden)\n",
    "    return tf.matmul(h3, enc4_w) + enc4_b\n",
    "\n",
    "ae1_w = tf.constant(sess.run(enc1_w)) #now ae1 weights are fixed, used for rbm layerwise training\n",
    "ae2_w = tf.constant(sess.run(enc2_w)) \n",
    "ae3_w = tf.constant(sess.run(enc3_w))\n",
    "ae4_w = tf.constant(sess.run(enc4_w))\n",
    "\n",
    "ae1_b = tf.constant(sess.run(enc1_b))\n",
    "ae2_b = tf.constant(sess.run(enc2_b))\n",
    "ae3_b = tf.constant(sess.run(enc3_b))\n",
    "ae4_b = tf.constant(sess.run(enc4_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X1 = tf.placeholder(\"float\", [None, 273])\n",
    "baseline = tf.placeholder(\"float\", [None, 39])\n",
    "\n",
    "p_keep_input = tf.placeholder(\"float\")\n",
    "p_keep_hidden = tf.placeholder(\"float\")\n",
    "\n",
    "shared_rep = encoder(X1, ae1_w, ae1_b, ae2_w, ae2_b, ae3_w, ae3_b, ae4_w, ae4_b, p_keep_input, p_keep_hidden)\n",
    "stacked = tf.concat(1, [shared_rep, baseline])\n",
    "\n",
    "transformed_train = sess.run(stacked, feed_dict = {X1: X1_tr, baseline: baseline_acoustic_tr, p_keep_input: 1.0, p_keep_hidden: 1.0})\n",
    "transformed_dev = sess.run(stacked, feed_dict = {X1: X1_dev, baseline: baseline_acoustic_dev, p_keep_input: 1.0, p_keep_hidden: 1.0})\n",
    "transformed_test = sess.run(stacked, feed_dict = {X1: X1_test, baseline: baseline_acoustic_test, p_keep_input: 1.0, p_keep_hidden: 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scipy.io.savemat('splitae_projected_data.mat', {'dataTr': transformed_train, 'PhonesTr': Phones_tr, 'dataDev': transformed_dev, 'PhonesDev': Phones_dev, 'dataTest': transformed_test, 'PhonesTest': Phones_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_fixed_w_train = tf.constant(sess.run(lr_w_o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.851466666667\n",
      "1 0.8538\n",
      "2 0.854533333333\n",
      "3 0.854466666667\n",
      "4 0.854066666667\n",
      "5 0.853866666667\n",
      "6 0.854266666667\n",
      "7 0.854266666667\n",
      "8 0.854066666667\n",
      "9 0.853133333333\n",
      "10 0.853266666667\n",
      "11 0.852866666667\n",
      "12 0.852866666667\n",
      "13 0.852866666667\n",
      "14 0.852866666667\n",
      "15 0.8528\n",
      "16 0.852466666667\n",
      "17 0.853\n",
      "18 0.8528\n",
      "19 0.852733333333\n",
      "20 0.852533333333\n",
      "21 0.8526\n",
      "22 0.852733333333\n",
      "23 0.852666666667\n",
      "24 0.852533333333\n",
      "25 0.852533333333\n",
      "26 0.8524\n",
      "27 0.852333333333\n",
      "28 0.852066666667\n",
      "29 0.852133333333\n",
      "30 0.8522\n",
      "31 0.852\n",
      "32 0.852133333333\n",
      "33 0.851866666667\n",
      "34 0.851933333333\n",
      "35 0.851466666667\n",
      "36 0.8518\n",
      "37 0.850333333333\n",
      "38 0.849133333333\n",
      "39 0.850533333333\n",
      "40 0.852133333333\n",
      "41 0.851666666667\n",
      "42 0.851133333333\n",
      "43 0.851666666667\n",
      "44 0.8514\n",
      "45 0.851666666667\n",
      "46 0.851733333333\n",
      "47 0.8518\n",
      "48 0.8518\n",
      "49 0.8516\n",
      "50 0.851533333333\n",
      "51 0.851733333333\n",
      "52 0.851933333333\n",
      "53 0.851933333333\n",
      "54 0.852066666667\n",
      "55 0.852266666667\n",
      "56 0.8522\n",
      "57 0.8522\n",
      "58 0.852066666667\n",
      "59 0.852066666667\n",
      "60 0.852133333333\n",
      "61 0.852\n",
      "62 0.8518\n",
      "63 0.851733333333\n",
      "64 0.851533333333\n",
      "65 0.851466666667\n",
      "66 0.851533333333\n",
      "67 0.851266666667\n",
      "68 0.8514\n",
      "69 0.851066666667\n",
      "70 0.850733333333\n",
      "71 0.850666666667\n",
      "72 0.8506\n",
      "73 0.8508\n",
      "74 0.850533333333\n",
      "75 0.850266666667\n",
      "76 0.850066666667\n",
      "77 0.8502\n",
      "78 0.850266666667\n",
      "79 0.8498\n",
      "80 0.8496\n",
      "81 0.849\n",
      "82 0.8476\n",
      "83 0.848933333333\n",
      "84 0.848066666667\n",
      "85 0.8494\n",
      "86 0.848933333333\n",
      "87 0.8486\n",
      "88 0.848533333333\n",
      "89 0.848666666667\n",
      "90 0.848733333333\n",
      "91 0.848933333333\n",
      "92 0.849\n",
      "93 0.848933333333\n",
      "94 0.849133333333\n",
      "95 0.8492\n",
      "96 0.849066666667\n",
      "97 0.8494\n",
      "98 0.849266666667\n",
      "99 0.8492\n",
      "LR on test data\n",
      "0.8484\n"
     ]
    }
   ],
   "source": [
    "def lr_model(X, baseline, w_o):\n",
    "    final = tf.concat(1, [X, baseline])\n",
    "    return tf.matmul(final, w_o)\n",
    "\n",
    "X1 = tf.placeholder(\"float\", [None, 273])\n",
    "Y = tf.placeholder(\"float\", [None, 39])\n",
    "baseline = tf.placeholder(\"float\", [None, 39])\n",
    "\n",
    "lr_w_o = init_weights([89, 39])\n",
    "\n",
    "p_keep_input = tf.placeholder(\"float\")\n",
    "p_keep_output = tf.placeholder(\"float\")\n",
    "\n",
    "shared = encoder(X1, ae1_w, ae1_b, ae2_w, ae2_b, ae3_w, ae3_b, ae4_w, ae4_b, p_keep_input, p_keep_hidden)\n",
    "py_x = lr_model(shared, baseline, lr_w_o)\n",
    "lr_cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(py_x, Y))\n",
    "lr_train_op = tf.train.AdamOptimizer(1e-3).minimize(lr_cost)\n",
    "\n",
    "predict_op = tf.argmax(py_x, 1)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    for start, end in zip(range(0, len(X1_tr), 100), range(100, len(X1_tr), 100)):\n",
    "        sess.run(lr_train_op, feed_dict = {X1: X1_tr[start:end], Y: Y_tr[start:end], baseline: baseline_acoustic_tr[start:end], p_keep_input: 1.0, p_keep_hidden: 1.0})\n",
    "    #print \"Epoch \", i, \". cost = \", sess.run(lr_cost, feed_dict = {X1: X1_tr[start:end], Y: Y_tr[start:end], baseline: baseline_acoustic_tr[start:end], p_keep_input: 1.0, p_keep_hidden: 1.0})\n",
    "    print i, np.mean(np.argmax(Y_dev, axis=1) == sess.run(predict_op, feed_dict = {X1: X1_dev, Y: Y_dev, baseline: baseline_acoustic_dev, p_keep_input: 1.0, p_keep_hidden: 1.0}))\n",
    "    \n",
    "print 'LR on test data'\n",
    "print np.mean(np.argmax(Y_test, axis=1) == sess.run(predict_op, feed_dict = {X1: X1_test, Y: Y_test, baseline: baseline_acoustic_test, p_keep_input: 1.0, p_keep_hidden: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
