{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy.io\n",
    "from sklearn import preprocessing\n",
    "from sklearn import cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fileidxJW11 = scipy.io.loadmat(\"XRMB/DATA/FILEIDX/fileidxJW11.mat\")\n",
    "fileidxJW13 = scipy.io.loadmat(\"XRMB/DATA/FILEIDX/fileidxJW13.mat\")\n",
    "fileidxJW24 = scipy.io.loadmat(\"XRMB/DATA/FILEIDX/fileidxJW24.mat\")\n",
    "fileidxJW30 = scipy.io.loadmat(\"XRMB/DATA/FILEIDX/fileidxJW30.mat\")\n",
    "\n",
    "JW11 = scipy.io.loadmat(\"XRMB/DATA/MAT/JW11[numfr1=7,numfr2=7].mat\")\n",
    "JW13 = scipy.io.loadmat(\"XRMB/DATA/MAT/JW13[numfr1=7,numfr2=7].mat\")\n",
    "JW24 = scipy.io.loadmat(\"XRMB/DATA/MAT/JW24[numfr1=7,numfr2=7].mat\")\n",
    "JW30 = scipy.io.loadmat(\"XRMB/DATA/MAT/JW30[numfr1=7,numfr2=7].mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bigindices', 'Valid_Files', 'indices', 'MFCC', 'Phones', 'bigP', '__header__', '__globals__', 'P', 'bigPhones', 'Words', 'frame_locs', 'Frames', 'X', '__version__', 'SpeakerID', 'Times']\n",
      "(15947, 273)\n",
      "(15947, 112)\n",
      "(15947, 39)\n",
      "(15947, 38)\n"
     ]
    }
   ],
   "source": [
    "print JW11.keys()\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "mfcc_features = scaler.fit_transform(preprocessing.normalize(np.transpose(JW11['MFCC'])))\n",
    "articulatory_features = scaler.fit_transform(preprocessing.normalize(np.transpose(JW11['X']).astype(float)))\n",
    "phone_labels = np.transpose(JW11['P'][0])\n",
    "\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(phone_labels)\n",
    "binarized_labels = lb.transform(phone_labels)\n",
    "\n",
    "n_samples = mfcc_features.shape[0]\n",
    "n_mfcc_features = mfcc_features.shape[1]\n",
    "n_articulatory_features = articulatory_features.shape[1]\n",
    "\n",
    "permutation = np.random.permutation(n_samples)\n",
    "X1 = np.asarray([mfcc_features[i] for i in permutation])\n",
    "X2 = np.asarray([articulatory_features[i] for i in permutation])\n",
    "Y = np.asarray([binarized_labels[i] for i in permutation])\n",
    "\n",
    "train, dev, test = 15948, 25948, 40948 #use 25948, 40948, 50948\n",
    "\n",
    "X1_tr = X1[1:train, :]\n",
    "X1_dev = X1[train+1:dev, :]\n",
    "X1_test = X1[dev+1:test, :]\n",
    "\n",
    "X2_tr = X2[1:train, :]\n",
    "\n",
    "Y_tr = Y[1:train, :]\n",
    "Y_dev = Y[train+1:dev, :]\n",
    "Y_test = Y[dev+1:test, :]\n",
    "\n",
    "baseline_acoustic_tr = X1_tr[:, 118:156]\n",
    "baseline_acoustic_dev = X1_dev[:, 118:156]\n",
    "baseline_acoustic_test = X1_test[:, 118:156]\n",
    "    \n",
    "print X1_tr.shape\n",
    "print X2_tr.shape\n",
    "print Y_tr.shape\n",
    "print baseline_acoustic_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=0.01))\n",
    "\n",
    "def model(X, w):\n",
    "    return tf.matmul(X, w)\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 273])\n",
    "Y = tf.placeholder(\"float\", [None, 39])\n",
    "\n",
    "w = init_weights([273, 39])\n",
    "\n",
    "py_x = model(X, w)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(py_x, Y))\n",
    "train_op = tf.train.GradientDescentOptimizer(0.001).minimize(cost)\n",
    "predict_op = tf.argmax(py_x, 1)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(100):\n",
    "    for start, end in zip(range(0, len(X1_tr), 100), range(n_mfcc_features, len(X1_tr), 100)):\n",
    "        sess.run(train_op, feed_dict = {X: X1_tr[start:end], Y: Y_tr[start:end]})\n",
    "    print i, np.mean(np.argmax(Y_dev, axis=1) == sess.run(predict_op, feed_dict = {X: X1_dev, Y: Y_dev}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " autoencoder training: \n",
      "done training autoencoder\n",
      "logistic regression training: \n",
      "0 0.268126812681\n",
      "1 0.343834383438\n",
      "2 0.425442544254\n",
      "3 0.46904690469\n",
      "4 0.500050005001\n",
      "5 0.5199519952\n",
      "6 0.539453945395\n",
      "7 0.552055205521\n",
      "8 0.57095709571\n",
      "9 0.579657965797\n",
      "10 0.603860386039\n",
      "11 0.614561456146\n",
      "12 0.624862486249\n",
      "13 0.640164016402\n",
      "14 0.637063706371\n",
      "15 0.641664166417\n",
      "16 0.641864186419\n",
      "17 0.663666366637\n",
      "18 0.661666166617\n",
      "19 0.670067006701\n",
      "20 0.67596759676\n",
      "21 0.686668666867\n",
      "22 0.687068706871\n",
      "23 0.687268726873\n",
      "24 0.688768876888\n",
      "25 0.686668666867\n",
      "26 0.705370537054\n",
      "27 0.699669966997\n",
      "28 0.693069306931\n",
      "29 0.694769476948\n",
      "30 0.700670067007\n",
      "31 0.69496949695\n",
      "32 0.706170617062\n",
      "33 0.703870387039\n",
      "34 0.705870587059\n",
      "35 0.708770877088\n",
      "36 0.711771177118\n",
      "37 0.704370437044\n",
      "38 0.707170717072\n",
      "39 0.708370837084\n",
      "40 0.712871287129\n",
      "41 0.701270127013\n",
      "42 0.708470847085\n",
      "43 0.707170717072\n",
      "44 0.704170417042\n",
      "45 0.716471647165\n",
      "46 0.712571257126\n",
      "47 0.719671967197\n",
      "48 0.701270127013\n",
      "49 0.70797079708\n",
      "50 0.725472547255\n",
      "51 0.715171517152\n",
      "52 0.71497149715\n",
      "53 0.715471547155\n",
      "54 0.706170617062\n",
      "55 0.725272527253\n",
      "56 0.726572657266\n",
      "57 0.728372837284\n",
      "58 0.732473247325\n",
      "59 0.718871887189\n",
      "60 0.714671467147\n",
      "61 0.727472747275\n",
      "62 0.731573157316\n",
      "63 0.723172317232\n",
      "64 0.72197219722\n",
      "65 0.723272327233\n",
      "66 0.724172417242\n",
      "67 0.729472947295\n",
      "68 0.723172317232\n",
      "69 0.734673467347\n",
      "70 0.740074007401\n",
      "71 0.719271927193\n",
      "72 0.736773677368\n",
      "73 0.73597359736\n",
      "74 0.734673467347\n",
      "75 0.7399739974\n",
      "76 0.735873587359\n",
      "77 0.728072807281\n",
      "78 0.728672867287\n",
      "79 0.730073007301\n",
      "80 0.726672667267\n",
      "81 0.74397439744\n",
      "82 0.737873787379\n",
      "83 0.733673367337\n",
      "84 0.737573757376\n",
      "85 0.742474247425\n",
      "86 0.729272927293\n",
      "87 0.734373437344\n",
      "88 0.734073407341\n",
      "89 0.717671767177\n",
      "90 0.737173717372\n",
      "91 0.740874087409\n",
      "92 0.739473947395\n",
      "93 0.74697469747\n",
      "94 0.746474647465\n",
      "95 0.737473747375\n",
      "96 0.739873987399\n",
      "97 0.735673567357\n",
      "98 0.737573757376\n",
      "99 0.747774777478\n",
      "100 0.735273527353\n",
      "101 0.745274527453\n",
      "102 0.734873487349\n",
      "103 0.749174917492\n",
      "104 0.728872887289\n",
      "105 0.744874487449\n",
      "106 0.752175217522\n",
      "107 0.753375337534\n",
      "108 0.749374937494\n",
      "109 0.753175317532\n",
      "110 0.730773077308\n",
      "111 0.736073607361\n",
      "112 0.746874687469\n",
      "113 0.745074507451\n",
      "114 0.747574757476\n",
      "115 0.748674867487\n",
      "116 0.744674467447\n",
      "117 0.754875487549\n",
      "118 0.748274827483\n",
      "119 0.740274027403\n",
      "120 0.740574057406\n",
      "121 0.737673767377\n",
      "122 0.74597459746\n",
      "123 0.742074207421\n",
      "124 0.755675567557\n",
      "125 0.753275327533\n",
      "126 0.746774677468\n",
      "127 0.750075007501\n",
      "128 0.742474247425\n",
      "129 0.740774077408\n",
      "130 0.743674367437\n",
      "131 0.728172817282\n",
      "132 0.744074407441\n",
      "133 0.753175317532\n",
      "134 0.736373637364\n",
      "135 0.752875287529\n",
      "136 0.752075207521\n",
      "137 0.751075107511\n",
      "138 0.738073807381\n",
      "139 0.743874387439\n",
      "140 0.738173817382\n",
      "141 0.751275127513\n",
      "142 0.753275327533\n",
      "143 0.750375037504\n",
      "144 0.74897489749\n",
      "145 0.745674567457\n",
      "146 0.757875787579\n",
      "147 0.749574957496\n",
      "148 0.746374637464\n",
      "149 0.744874487449\n",
      "150 0.73897389739\n",
      "151 0.731073107311\n",
      "152 0.742474247425\n",
      "153 0.747074707471\n",
      "154 0.750075007501\n",
      "155 0.758475847585\n",
      "156 0.752675267527\n",
      "157 0.751075107511\n",
      "158 0.753175317532\n",
      "159 0.745374537454\n",
      "160 0.748374837484\n",
      "161 0.755775577558\n",
      "162 0.759675967597\n",
      "163 0.761876187619\n",
      "164 0.766376637664\n",
      "165 0.755775577558\n",
      "166 0.745774577458\n",
      "167 0.74497449745\n",
      "168 0.744274427443\n",
      "169 0.744074407441\n",
      "170 0.749174917492\n",
      "171 0.742674267427\n",
      "172 0.740274027403\n",
      "173 0.741874187419\n",
      "174 0.757575757576\n",
      "175 0.752775277528\n",
      "176 0.760076007601\n",
      "177 0.745474547455\n",
      "178 0.741274127413\n",
      "179 0.753475347535\n",
      "180 0.746174617462\n",
      "181 0.752175217522\n",
      "182 0.755675567557\n",
      "183 0.74897489749\n",
      "184 0.757575757576\n",
      "185 0.74797479748\n",
      "186 0.752575257526\n",
      "187 0.755275527553\n",
      "188 0.749674967497\n",
      "189 0.746174617462\n",
      "190 0.745074507451\n",
      "191 0.754475447545\n",
      "192 0.758375837584\n",
      "193 0.759475947595\n",
      "194 0.743574357436\n",
      "195 0.757575757576\n",
      "196 0.753175317532\n",
      "197 0.752275227523\n",
      "198 0.755375537554\n",
      "199 0.761576157616\n",
      "200 0.763576357636\n",
      "201 0.75697569757\n",
      "202 0.754175417542\n",
      "203 0.761376137614\n",
      "204 0.75797579758\n",
      "205 0.7499749975\n",
      "206 0.754475447545\n",
      "207 0.756175617562\n",
      "208 0.762876287629\n",
      "209 0.752875287529\n",
      "210 0.744574457446\n",
      "211 0.746274627463\n",
      "212 0.752175217522\n",
      "213 0.75497549755\n",
      "214 0.763076307631\n",
      "215 0.760576057606\n",
      "216 0.756775677568\n",
      "217 0.750875087509\n",
      "218 0.757375737574\n",
      "219 0.760676067607\n",
      "220 0.748674867487\n",
      "221 0.748874887489\n",
      "222 0.742574257426\n",
      "223 0.736673667367\n",
      "224 0.749374937494\n",
      "225 0.753275327533\n",
      "226 0.760576057606\n",
      "227 0.757175717572\n",
      "228 0.754875487549\n",
      "229 0.762376237624\n",
      "230 0.757575757576\n",
      "231 0.762376237624\n",
      "232 0.754175417542\n",
      "233 0.755375537554\n",
      "234 0.753875387539\n",
      "235 0.757175717572\n",
      "236 0.748874887489\n",
      "237 0.7499749975\n",
      "238 0.751475147515\n",
      "239 0.75797579758\n",
      "240 0.752675267527\n",
      "241 0.747374737474\n",
      "242 0.75097509751\n",
      "243 0.749074907491\n",
      "244 0.758375837584\n",
      "245 0.760276027603\n",
      "246 0.761476147615\n",
      "247 0.765776577658\n",
      "248 0.76597659766\n",
      "249 0.764376437644\n",
      "250 0.759575957596\n",
      "251 0.7499749975\n",
      "252 0.752375237524\n",
      "253 0.756875687569\n",
      "254 0.759575957596\n",
      "255 0.749574957496\n",
      "256 0.754575457546\n",
      "257 0.759875987599\n",
      "258 0.759675967597\n",
      "259 0.763176317632\n",
      "260 0.757375737574\n",
      "261 0.754075407541\n",
      "262 0.762176217622\n",
      "263 0.762676267627\n",
      "264 0.759675967597\n",
      "265 0.745274527453\n",
      "266 0.75697569757\n",
      "267 0.763576357636\n",
      "268 0.752275227523\n",
      "269 0.753875387539\n",
      "270 0.753275327533\n",
      "271 0.75697569757\n",
      "272 0.755375537554\n",
      "273 0.763576357636\n",
      "274 0.758475847585\n",
      "275 0.747374737474\n",
      "276 0.751175117512\n",
      "277 0.759575957596\n",
      "278 0.757175717572\n",
      "279 0.755075507551\n",
      "280 0.746874687469\n",
      "281 0.757675767577\n",
      "282 0.762076207621\n",
      "283 0.756475647565\n",
      "284 0.761776177618\n",
      "285 0.76397639764\n",
      "286 0.763576357636\n",
      "287 0.760876087609\n",
      "288 0.764776477648\n",
      "289 0.75697569757\n",
      "290 0.750475047505\n",
      "291 0.758275827583\n",
      "292 0.746574657466\n",
      "293 0.750875087509\n",
      "294 0.753075307531\n",
      "295 0.755475547555\n",
      "296 0.761876187619\n",
      "297 0.760576057606\n",
      "298 0.762876287629\n",
      "299 0.767776777678\n",
      "300 0.759375937594\n",
      "301 0.759075907591\n",
      "302 0.763876387639\n",
      "303 0.751875187519\n",
      "304 0.74497449745\n",
      "305 0.757275727573\n",
      "306 0.758675867587\n",
      "307 0.756275627563\n",
      "308 0.750375037504\n",
      "309 0.746474647465\n",
      "310 0.749274927493\n",
      "311 0.757275727573\n",
      "312 0.76697669767\n",
      "313 0.753275327533\n",
      "314 0.761576157616\n",
      "315 0.772677267727\n",
      "316 0.768476847685\n",
      "317 0.765776577658\n",
      "318 0.767076707671\n",
      "319 0.760276027603\n",
      "320 0.733673367337\n",
      "321 0.745174517452\n",
      "322 0.751175117512\n",
      "323 0.757575757576\n",
      "324 0.76497649765\n",
      "325 0.754575457546\n",
      "326 0.760276027603\n",
      "327 0.761376137614\n",
      "328 0.763176317632\n",
      "329 0.761176117612\n",
      "330 0.75697569757\n",
      "331 0.758075807581\n",
      "332 0.758575857586\n",
      "333 0.75297529753\n",
      "334 0.758475847585\n",
      "335 0.768476847685\n",
      "336 0.760776077608\n",
      "337 0.761076107611\n",
      "338 0.763776377638\n",
      "339 0.746374637464\n",
      "340 0.744274427443\n",
      "341 0.740274027403\n",
      "342 0.753075307531\n",
      "343 0.756875687569\n",
      "344 0.762276227623\n",
      "345 0.759475947595\n",
      "346 0.756775677568\n",
      "347 0.758875887589\n",
      "348 0.76097609761\n",
      "349 0.770377037704\n",
      "350 0.765276527653\n",
      "351 0.767676767677\n",
      "352 0.762076207621\n",
      "353 0.759475947595\n",
      "354 0.753275327533\n",
      "355 0.752075207521\n",
      "356 0.730473047305\n",
      "357 0.752075207521\n",
      "358 0.74897489749\n",
      "359 0.755075507551\n",
      "360 0.74797479748\n",
      "361 0.754675467547\n",
      "362 0.754675467547\n",
      "363 0.755275527553\n",
      "364 0.750375037504\n",
      "365 0.757175717572\n",
      "366 0.763376337634\n",
      "367 0.762376237624\n",
      "368 0.753675367537\n",
      "369 0.753775377538\n",
      "370 0.762176217622\n",
      "371 0.732773277328\n",
      "372 0.754675467547\n",
      "373 0.752875287529\n",
      "374 0.755875587559\n",
      "375 0.752575257526\n",
      "376 0.754575457546\n",
      "377 0.756075607561\n",
      "378 0.742774277428\n",
      "379 0.751375137514\n",
      "380 0.752575257526\n",
      "381 0.759175917592\n",
      "382 0.753275327533\n",
      "383 0.761376137614\n",
      "384 0.75597559756\n",
      "385 0.761076107611\n",
      "386 0.761376137614\n",
      "387 0.749374937494\n",
      "388 0.757675767577\n",
      "389 0.760576057606\n",
      "390 0.76497649765\n",
      "391 0.759075907591\n",
      "392 0.764376437644\n",
      "393 0.761576157616\n",
      "394 0.75097509751\n",
      "395 0.749474947495\n",
      "396 0.762276227623\n",
      "397 0.759775977598\n",
      "398 0.754775477548\n",
      "399 0.749274927493\n",
      "400 0.747274727473\n",
      "401 0.75497549755\n",
      "402 0.756875687569\n",
      "403 0.748674867487\n",
      "404 0.757375737574\n",
      "405 0.760676067607\n",
      "406 0.760776077608\n",
      "407 0.756475647565\n",
      "408 0.76097609761\n",
      "409 0.763576357636\n",
      "410 0.760576057606\n",
      "411 0.754275427543\n",
      "412 0.763176317632\n",
      "413 0.757075707571\n",
      "414 0.75397539754\n",
      "415 0.755875587559\n",
      "416 0.760476047605\n",
      "417 0.748174817482\n",
      "418 0.7499749975\n",
      "419 0.750075007501\n",
      "420 0.754075407541\n",
      "421 0.75697569757\n",
      "422 0.751475147515\n",
      "423 0.76797679768\n",
      "424 0.763676367637\n",
      "425 0.767376737674\n",
      "426 0.770877087709\n",
      "427 0.767576757676\n",
      "428 0.750275027503\n",
      "429 0.754875487549\n",
      "430 0.74597459746\n",
      "431 0.754075407541\n",
      "432 0.752575257526\n",
      "433 0.747574757476\n",
      "434 0.755275527553\n",
      "435 0.759775977598\n",
      "436 0.761176117612\n",
      "437 0.755575557556\n",
      "438 0.746174617462\n",
      "439 0.748774877488\n",
      "440 0.751275127513\n",
      "441 0.762076207621\n",
      "442 0.750875087509\n",
      "443 0.755775577558\n",
      "444 0.746674667467\n",
      "445 0.749674967497\n",
      "446 0.75397539754\n",
      "447 0.753375337534\n",
      "448 0.743174317432\n",
      "449 0.750175017502\n",
      "450 0.75197519752\n",
      "451 0.756375637564\n",
      "452 0.753275327533\n",
      "453 0.760176017602\n",
      "454 0.761476147615\n",
      "455 0.762076207621\n",
      "456 0.762576257626\n",
      "457 0.760076007601\n",
      "458 0.754875487549\n",
      "459 0.748374837484\n",
      "460 0.754875487549\n",
      "461 0.746074607461\n",
      "462 0.734373437344\n",
      "463 0.74497449745\n",
      "464 0.75397539754\n",
      "465 0.750175017502\n",
      "466 0.748474847485\n",
      "467 0.75397539754\n",
      "468 0.754175417542\n",
      "469 0.748374837484\n",
      "470 0.759875987599\n",
      "471 0.751375137514\n",
      "472 0.75097509751\n",
      "473 0.753275327533\n",
      "474 0.752275227523\n",
      "475 0.746874687469\n",
      "476 0.752675267527\n",
      "477 0.754875487549\n",
      "478 0.742474247425\n",
      "479 0.742174217422\n",
      "480 0.745874587459\n",
      "481 0.747074707471\n",
      "482 0.754375437544\n",
      "483 0.747874787479\n",
      "484 0.751075107511\n",
      "485 0.749474947495\n",
      "486 0.759075907591\n",
      "487 0.750675067507\n",
      "488 0.749774977498\n",
      "489 0.752275227523\n",
      "490 0.732873287329\n",
      "491 0.7499749975\n",
      "492 0.751675167517\n",
      "493 0.74697469747\n",
      "494 0.75297529753\n",
      "495 0.747574757476\n",
      "496 0.755275527553\n",
      "497 0.74897489749\n",
      "498 0.748474847485\n",
      "499 0.74497449745\n"
     ]
    }
   ],
   "source": [
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=0.01))\n",
    "\n",
    "def lr_model(X, baseline, w_h, w_o):\n",
    "    final = tf.concat(1, [X, baseline])\n",
    "    h = tf.nn.sigmoid(tf.matmul(final, w_h))\n",
    "    return tf.matmul(h, w_o)\n",
    "\n",
    "def encoder(X, enc1_w, enc1_b, enc2_w, enc2_b, enc3_w, enc3_b, enc4_w, enc4_b, p_keep_input, p_keep_hidden):\n",
    "    X = tf.nn.dropout(X, p_keep_input)\n",
    "    h1 = tf.nn.relu(tf.matmul(X, enc1_w) + enc1_b)\n",
    "    \n",
    "    h1 = tf.nn.dropout(h1, p_keep_hidden)\n",
    "    h2 = tf.nn.relu(tf.matmul(h1, enc2_w) + enc2_b)\n",
    "    \n",
    "    h2 = tf.nn.dropout(h2, p_keep_hidden)\n",
    "    h3 = tf.nn.relu(tf.matmul(h2, enc3_w) + enc3_b)\n",
    "    \n",
    "    h3 = tf.nn.dropout(h3, p_keep_hidden)\n",
    "    return tf.matmul(h3, enc4_w) + enc4_b\n",
    "    \n",
    "def decoder_X1(h4, dec1_w, dec1_b, dec2_w, dec2_b, dec3_w, dec3_b, dec4_w, dec4_b, p_keep_hidden):\n",
    "    h4 = tf.nn.dropout(h4, p_keep_hidden)\n",
    "    h3 = tf.nn.relu(tf.matmul(h4, dec4_w) + dec4_b)\n",
    "    \n",
    "    h3 = tf.nn.dropout(h3, p_keep_hidden)\n",
    "    h2 = tf.nn.relu(tf.matmul(h3, dec3_w) + dec3_b)\n",
    "    \n",
    "    h2 = tf.nn.dropout(h2, p_keep_hidden)\n",
    "    h1 = tf.nn.relu(tf.matmul(h2, dec2_w) + dec2_b)\n",
    "    \n",
    "    h1 = tf.nn.dropout(h1, p_keep_hidden)\n",
    "    return tf.matmul(h1, dec1_w) + dec1_b\n",
    "\n",
    "def decoder_X2(h4, dec4_w, dec4_b, p_keep_hidden):\n",
    "    h4 = tf.nn.dropout(h4, p_keep_hidden)\n",
    "    return tf.matmul(h4, dec4_w) + dec4_b\n",
    "\n",
    "# def decoder_X2(h4, dec1_w, dec1_b, dec2_w, dec2_b, dec3_w, dec3_b, dec4_w, dec4_b, p_keep_hidden):\n",
    "#     h4 = tf.nn.dropout(h4, p_keep_hidden)\n",
    "#     h3 = tf.nn.sigmoid(tf.matmul(h4, dec4_w) + dec4_b)\n",
    "    \n",
    "#     h3 = tf.nn.dropout(h3, p_keep_hidden)\n",
    "#     h2 = tf.nn.sigmoid(tf.matmul(h3, dec3_w) + dec3_b)\n",
    "    \n",
    "#     h2 = tf.nn.dropout(h2, p_keep_hidden)\n",
    "#     h1 = tf.nn.sigmoid(tf.matmul(h2, dec2_w) + dec2_b)\n",
    "    \n",
    "#     h1 = tf.nn.dropout(h1, p_keep_hidden)\n",
    "#     return tf.nn.sigmoid(tf.matmul(h1, dec1_w) + dec1_b)\n",
    "\n",
    "X1 = tf.placeholder(\"float\", [None, 273])\n",
    "X2 = tf.placeholder(\"float\", [None, 112])\n",
    "Y = tf.placeholder(\"float\", [None, 39])\n",
    "\n",
    "enc1_w = init_weights([273, 300])\n",
    "enc2_w = init_weights([300, 150])\n",
    "enc3_w = init_weights([150, 100])\n",
    "enc4_w = init_weights([100, 50])\n",
    "\n",
    "enc1_b = init_weights([1, 300])\n",
    "enc2_b = init_weights([1, 150])\n",
    "enc3_b = init_weights([1, 100])\n",
    "enc4_b = init_weights([1, 50])\n",
    "\n",
    "dec1_w_x1 = init_weights([300, 273])\n",
    "dec2_w_x1 = init_weights([150, 300])\n",
    "dec3_w_x1 = init_weights([100, 150])\n",
    "dec4_w_x1 = init_weights([50, 100])\n",
    "\n",
    "# dec1_w_x2 = init_weights([90, 112])\n",
    "# dec2_w_x2 = init_weights([65, 90])\n",
    "# dec3_w_x2 = init_weights([65, 65])\n",
    "dec4_w_x2 = init_weights([50, 112])\n",
    "\n",
    "dec1_b_x1 = init_weights([1, 273])\n",
    "dec2_b_x1 = init_weights([1, 300])\n",
    "dec3_b_x1 = init_weights([1, 150])\n",
    "dec4_b_x1 = init_weights([1, 100])\n",
    "\n",
    "# dec1_b_x2 = init_weights([1, 112])\n",
    "# dec2_b_x2 = init_weights([1, 90])\n",
    "# dec3_b_x2 = init_weights([1, 65])\n",
    "dec4_b_x2 = init_weights([1, 112])\n",
    "\n",
    "lr_w = init_weights([88, 50])\n",
    "lr_w_o = init_weights([50, 39])\n",
    "# lr_b = init_weights([1, 50])\n",
    "# lr_b_o = init_weights([1, 39])\n",
    "\n",
    "p_keep_input = tf.placeholder(\"float\")\n",
    "p_keep_hidden = tf.placeholder(\"float\")\n",
    "\n",
    "construction_x1 = encoder(X1, enc1_w, enc1_b, enc2_w, enc2_b, enc3_w, enc3_b, enc4_w, enc4_b, p_keep_input, p_keep_hidden)\n",
    "reconstruction_x1 = decoder_X1(construction_x1, dec1_w_x1, dec1_b_x1, dec2_w_x1, dec2_b_x1, dec3_w_x1, dec3_b_x1, dec4_w_x1, dec4_b_x1, p_keep_hidden)\n",
    "# reconstruction_x2 = decoder_X2(construction_x1, dec1_w_x2, dec1_b_x2, dec2_w_x2, dec2_b_x2, dec3_w_x2, dec3_b_x2, dec4_w_x2, dec4_b_x2, p_keep_hidden)\n",
    "reconstruction_x2 = decoder_X2(construction_x1, dec4_w_x2, dec4_b_x2, p_keep_hidden)\n",
    "\n",
    "ae_cost = tf.nn.l2_loss(reconstruction_x1 - X1) + tf.nn.l2_loss(reconstruction_x2 - X2)\n",
    "ae_train_op = tf.train.RMSPropOptimizer(1e-4, 0.9).minimize(ae_cost)\n",
    "#ae_train_op = tf.train.AdamOptimizer(0.001).minimize(ae_cost)\n",
    "\n",
    "baseline = tf.placeholder(\"float\", [None, 38])\n",
    "py_x = lr_model(construction_x1, baseline, lr_w, lr_w_o)\n",
    "lr_cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(py_x, Y))\n",
    "lr_train_op = tf.train.AdamOptimizer(0.001).minimize(lr_cost)\n",
    "\n",
    "predict_op = tf.argmax(py_x, 1)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "\n",
    "num_epochs = 500\n",
    "\n",
    "print \"autoencoder training: \"\n",
    "for i in range(num_epochs):\n",
    "    for start, end in zip(range(0, len(X1_tr), 100), range(100, len(X1_tr), 100)):\n",
    "        sess.run(ae_train_op, feed_dict = {X1: X1_tr[start:end], X2: X2_tr[start:end], Y: Y_tr[start:end], p_keep_input: 0.8, p_keep_hidden: 0.5})\n",
    "    #print \"Epoch \", i, \". Cost = \", sess.run(ae_cost, feed_dict = {X1: X1_tr, X2: X2_tr, Y: Y_tr, p_keep_input: 1.0, p_keep_hidden: 1.0})\n",
    "    #print \"\" + str(i) + \", \",\n",
    "print \"done training autoencoder\"\n",
    "        \n",
    "print \"logistic regression training: \"\n",
    "for i in range(500):\n",
    "    for start, end in zip(range(0, len(X1_tr), 100), range(100, len(X1_tr), 100)):\n",
    "        sess.run(lr_train_op, feed_dict = {X1: X1_tr[start:end], Y: Y_tr[start:end], baseline: baseline_acoustic_tr[start:end], p_keep_input: 1.0, p_keep_hidden: 1.0})\n",
    "    #print \"Epoch \", i, \". cost = \", sess.run(lr_cost, feed_dict = {X1: X1_tr[start:end], Y: Y_tr[start:end], baseline: baseline_acoustic_tr[start:end], p_keep_input: 1.0, p_keep_hidden: 1.0})\n",
    "    print i, np.mean(np.argmax(Y_dev, axis=1) == sess.run(predict_op, feed_dict = {X1: X1_dev, Y: Y_dev, baseline: baseline_acoustic_dev, p_keep_input: 1.0, p_keep_hidden: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
